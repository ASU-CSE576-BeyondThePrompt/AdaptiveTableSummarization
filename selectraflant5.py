# -*- coding: utf-8 -*-
"""SelectraFlant5

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1ct23UmwVqE7kgWhDllZzBnyhb6eFbqXX
"""

import json
from transformers import AutoTokenizer, AutoModelForSeq2SeqLM

# Step 1: Load FLAN-T5 (small for speed, use base/large for better accuracy)
model_name = "google/flan-t5-small"
tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSeq2SeqLM.from_pretrained(model_name)

# Step 2: Prompting function
def generate_user_preference(question):
    prompt = f"""Classify the user who asked this financial question:
Question: "{question}"
User type (choose one): financial analyst, business manager, technical expert, novice user, investor.
Answer:"""
    inputs = tokenizer(prompt, return_tensors="pt", truncation=True)
    outputs = model.generate(**inputs, max_new_tokens=10)
    return tokenizer.decode(outputs[0], skip_special_tokens=True).strip()

# Step 3: Load the TAT-QA dataset
tatqa_path = "tatqa_dataset_test_gold.json"
with open(tatqa_path, "r") as f:
    data = json.load(f)

# Step 4: Classify questions
user_preferences = []
for item in data:
    for q in item["questions"]:
        user_type = generate_user_preference(q["question"])
        user_preferences.append({
            "question": q["question"],
            "predicted_user_type": user_type
        })

# Step 5: Print results
print("\n--- Sample Predictions ---")
for entry in user_preferences[:10]:
    print(f"Q: {entry['question']}\nâ†’ Predicted User Type: {entry['predicted_user_type']}\n")